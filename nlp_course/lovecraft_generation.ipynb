{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lovecraft_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "163QiTE6HEZrQiSvDAquTkiKEH56L7qVe",
      "authorship_tag": "ABX9TyNmobaFSMxY50XDUUYKDYHh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parse data and clean it"
      ],
      "metadata": {
        "id": "10KZF4qZlGdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "IeXJ4W6ik5YP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31dd8c42-044a-47e4-87e0-79ffabddecf8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CleanHTML(object):\n",
        "\n",
        "    def __init__(self, html):\n",
        "        self.html_ = str(html)\n",
        "        self.soup_ = None\n",
        "\n",
        "    def get_soup(self):\n",
        "        return self.soup_\n",
        "\n",
        "    def process_html_tags(self, safe=None, unsafe=None):\n",
        "        SAFE_TAGS = []\n",
        "\n",
        "        safe = safe or SAFE_TAGS.copy()\n",
        "\n",
        "        extract = ['script', 'style']\n",
        "        save_attrs = ['highlight', 'iframe']\n",
        "\n",
        "        safe = set(safe) - set(unsafe or [])\n",
        "\n",
        "        soup = BeautifulSoup(self.html_, 'html5lib')\n",
        "\n",
        "        for tag in soup.findAll():\n",
        "            if tag.name not in save_attrs:\n",
        "                tag.attrs = []\n",
        "\n",
        "            if tag.name.lower() in extract:\n",
        "                tag.extract()\n",
        "\n",
        "            elif not tag.name.lower() in safe:\n",
        "                tag.replaceWithChildren()\n",
        "        self.soup_ = str(soup)"
      ],
      "metadata": {
        "id": "xt-KDtp6rggI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "\n",
        "def clean_html(text):\n",
        "    try:\n",
        "        if (text):\n",
        "            tmp_obj = CleanHTML(text)\n",
        "            tmp_obj.process_html_tags(None, None)\n",
        "            tmp_var = tmp_obj.get_soup()\n",
        "            # remove reference\n",
        "            tmp_obj = None\n",
        "            return tmp_var\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as ex:\n",
        "        raise ex\n",
        "\n",
        "def remove_url_if_possible(text):\n",
        "    if (text):\n",
        "        result = re.sub(\n",
        "            r'(http|ftp|https):\\/\\/([\\w\\-_]+(?:(?:\\.[\\w\\-_]+)+))([\\w\\-\\.,@?^=%&:/~\\+#]*[\\w\\-\\@?^=%&/~\\+#])?',\n",
        "            ' ', text)\n",
        "        return result\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def remove_multy_spaces(text):\n",
        "    try:\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text\n",
        "    except Exception as ex:\n",
        "        return None"
      ],
      "metadata": {
        "id": "2p7USU8hrrqu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_url = 'https://www.hplovecraft.com/writings/fiction/'\n",
        "texts_url = 'https://www.hplovecraft.com/writings/texts/fiction'"
      ],
      "metadata": {
        "id": "FhFih0PelCH9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body = requests.post(main_url).text"
      ],
      "metadata": {
        "id": "nRam8twUlMFt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(body)"
      ],
      "metadata": {
        "id": "bY8EJqCJlNn7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = []\n",
        "for h in soup.findAll('li'):\n",
        "    \n",
        "    a = h.find('a')\n",
        "    try:\n",
        "          \n",
        "        if 'href' in a.attrs:\n",
        "              \n",
        "            url = a.get('href')\n",
        "              \n",
        "            urls.append(url)\n",
        "    except:\n",
        "        pass\n",
        "urls = [os.path.join(texts_url,i) for i in urls]"
      ],
      "metadata": {
        "id": "Injt_RA-leJS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(urls)"
      ],
      "metadata": {
        "id": "cbZF0Pbkm2YD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a0425b-dea0-4ee8-8631-5c2f268a16e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_url_body(url):\n",
        "  body = requests.post(url).text\n",
        "  return body"
      ],
      "metadata": {
        "id": "_DqNXIoWl9hO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with ThreadPool(6) as pool:\n",
        "  parsed = list(tqdm(pool.imap(get_url_body, urls), total=len(urls)))"
      ],
      "metadata": {
        "id": "vW9GSmIfmU8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f4bd19-71eb-4649-f281-87eab3877e44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 105/105 [00:05<00:00, 20.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_text(parsed):\n",
        "  soup = BeautifulSoup(parsed)\n",
        "  text = None\n",
        "  for i in soup.findAll('tr'):\n",
        "     res = i.find('div', attrs={'align':\"justify\"})\n",
        "     if res:\n",
        "       text = remove_url_if_possible(clean_html(res)).strip()\n",
        "       break\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "MTNE-cLOpGtr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [parse_text(i) for i in parsed]\n",
        "texts = [i for i in texts if not(i is None)]"
      ],
      "metadata": {
        "id": "BVe9QcH-pKz8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [remove_multy_spaces(i) for i in texts]"
      ],
      "metadata": {
        "id": "yoZl9cQ0qbK7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [sent_tokenize(i) for i in texts]"
      ],
      "metadata": {
        "id": "-kPPCeI1u2bH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min([len(i) for i in sentences]), max([len(i) for i in sentences])"
      ],
      "metadata": {
        "id": "JGfAavgowCky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b07d90b2-b73f-4069-ffad-682ef8b94722"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 1751)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning GPT2"
      ],
      "metadata": {
        "id": "58tZpv0YtuLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "1t1zdVSTt0gm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b35f38-c1f5-4b60-82d3-6a24b7868b51"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 29.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 26.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 70.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "1uvh_rqvtboT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "jStqY99YtyQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba0cd8c-ee8b-4a90-c27e-52c193f8a0d9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul 30 06:31:40 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    10W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n"
      ],
      "metadata": {
        "id": "ysNH-Ma3uL7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0efaf74-c684-426b-89c8-442fbfc21498"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe96ede07f0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', bos_token='<start>',\n",
        "                                          eos_token='<end>', pad_token='<pad>')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium').cuda()\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "iDR79i-kubtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01cf0258-9a1c-49b8-8cdd-611680d35a90"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50260, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = np.hstack(sentences)"
      ],
      "metadata": {
        "id": "Ecvh4ZAYvdWx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(tokenizer.encode(sentence)) for sentence in sentences])\n"
      ],
      "metadata": {
        "id": "Ep_89s05uj41"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length"
      ],
      "metadata": {
        "id": "4NCy5zSGwps2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29919c80-dc4a-4e1e-f355-27484f96cb4f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "299"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LovecraftDataset(Dataset):\n",
        "    def __init__(self, txt_list, tokenizer, max_length\n",
        "                 ):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        for txt in txt_list:\n",
        "            encodings_dict = tokenizer('<start>' + txt + '<end>', truncation=True,\n",
        "                                       max_length=max_length, padding=\"max_length\")\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]"
      ],
      "metadata": {
        "id": "zLYxBOdEwq1O"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = LovecraftDataset(sentences, tokenizer, max_length=max_length)\n"
      ],
      "metadata": {
        "id": "sez3-oE0xIUY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.9 * len(dataset))\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])"
      ],
      "metadata": {
        "id": "s6hGrXOwxVTa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc \n",
        "gc.collect();\n"
      ],
      "metadata": {
        "id": "ohyzovPozHpl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(output_dir='drive/MyDrive/model_gpt_lovecraft', num_train_epochs=2, logging_steps=1000, save_steps=10000,\n",
        "                                  per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
        "                                  warmup_steps=100, weight_decay=0.06, logging_dir='./logs', report_to = 'none')"
      ],
      "metadata": {
        "id": "q4EMvV4oxogW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n",
        "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
        "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
        "                                                              'labels': torch.stack([f[0] for f in data])}).train()"
      ],
      "metadata": {
        "id": "3t3HSxqKxwgz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65c5e1db-dcf4-479d-c4fe-7788fb5487b8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 25799\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 25799\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25799' max='25799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25799/25799 2:54:48, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.644300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.471600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.441700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.432400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.427100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.424900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.421100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.433400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.433100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.428200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.419100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.434800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.409900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.405500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.418300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.425000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.413500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.423300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.414700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.402300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.408400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.415900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.408500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.408200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.389900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to drive/MyDrive/model_gpt_lovecraft/checkpoint-10000\n",
            "Configuration saved in drive/MyDrive/model_gpt_lovecraft/checkpoint-10000/config.json\n",
            "Model weights saved in drive/MyDrive/model_gpt_lovecraft/checkpoint-10000/pytorch_model.bin\n",
            "Saving model checkpoint to drive/MyDrive/model_gpt_lovecraft/checkpoint-20000\n",
            "Configuration saved in drive/MyDrive/model_gpt_lovecraft/checkpoint-20000/config.json\n",
            "Model weights saved in drive/MyDrive/model_gpt_lovecraft/checkpoint-20000/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=25799, training_loss=0.4293358824715983, metrics={'train_runtime': 10489.7958, 'train_samples_per_second': 2.459, 'train_steps_per_second': 2.459, 'total_flos': 1.3992002375430144e+16, 'train_loss': 0.4293358824715983, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate"
      ],
      "metadata": {
        "id": "Kd2FvUPLxtft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('drive/MyDrive/model_gpt_lovecraft/checkpoint-10000',\n",
        "                                        local_files_only=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', bos_token='<start>',\n",
        "                                          eos_token='<end>', pad_token='<pad>')"
      ],
      "metadata": {
        "id": "qWi594baxz0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4a9950-5c93-4723-a154-ae1dd886e70e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = tokenizer(\"<start> \", return_tensors=\"pt\").input_ids.cpu()\n"
      ],
      "metadata": {
        "id": "OhY2GhdSyPpp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = model.generate(generated, do_sample=True, top_k=50, \n",
        "                                max_length=100, top_p=0.95, temperature=1.9, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx1LR5mIybB3",
        "outputId": "38c22fd5-d259-475e-ff1b-17868a329625"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "    print('*'*30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9GXzvcPydhm",
        "outputId": "f9b15a73-1893-4a7e-e205-c04a0afb2f28"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:  ~~~~~~~~.” The old man paused as Wilcox made an appearance with two others. 3 copies were given each, accompanied on each one by a blank blank message printed out by Joseph Curwen, Phaesarian 0 copies of those earlier copies received. ~~~~~~~~ ——— It was then that the two\n",
            "******************************\n",
            "1:  ‘It’s the right place for men”—as the police and district officers now agree— and that I know where to bring home the men so you’ll know when there are too dangerous for the local criminals to handle. In these parts there seem more such houses and shops now than formerly..—and..” Mevana came within an opening in these walls when\n",
            "******************************\n",
            "2:  ’Nyl ‘Tohama, the way?” “And, for a boy, I cannot go up a ladder of even my size. d? ‘da da da dazhin? d’ autodecabatas. de * * * ** * ‣da da dazhin?” * * * * Ummm,\n",
            "******************************\n",
            "3:  ’Bare to our heads must go is thy life shall go. ATD Nd auß ATd nn Nu. DN DN –AT’N’S A DE M D E PEUN’C DE M E X Nd Nn au la. N N 1 M M 1 1 A OFFER.\n",
            "******************************\n",
            "4:  iawaaaa, they’re aaout the rest o’ dang their own parts fer people here in ye village—we know where all the fisshest is, for y’know we hear there’m fer yer old fannie folks’ and everybody, old men that a lot oem do ’n hear o’ Yew and Yogs..... Aout to\n",
            "******************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = tokenizer(\"The ancients\", return_tensors=\"pt\").input_ids.cpu()\n"
      ],
      "metadata": {
        "id": "Caqxm174ddgX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = model.generate(generated, dtop_p=0.95, temperature=1.9, do_sample=True,\n",
        "                                max_length=100, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV9KK23_yu_H",
        "outputId": "6e2bce2e-7ffd-4561-e8b5-307540ce935a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "    print('*'*30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu1q-lslcwHb",
        "outputId": "76306b21-3e5b-4339-cf5b-fccad8dbe221"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: The ancients are so vain’ to find time—for in this world the time you do know or know how is not at once—I suppose that our generation�ks have had that idea about time. The picture has ceased to exist—a useless but incredibly accurate and faithful copy of some image of our own.” And suddenly I screamed aloud and became frightened, for before my mind and my mind only were still frozen in horror\n",
            "******************************\n",
            "1: The ancients called forth from the cavern depths above a dark, nebous fear, saying, that they had felt as bad, and did all them bad with wilders like wild dogs which grow in love too much into cats; and that something was, or had formerly been thought within their ranks..,! “Oh dear,” began Nodens\n",
            "******************************\n",
            "2: The ancients who saw him are silent, and all of one heart; because that is what has told, and those are the men behind that whisper—those are those whines and murmure whereon many have forgotten the tale, what things of an early generation are known, what legends and folk legends hold the terrible memories for ever and ever., not a human being has come upon the\n",
            "******************************\n",
            "3: The ancients have sought as strange a dwelling in their days as the sea or what their own ocean holds—all such seek must, I was say at another time and more clearly, return up the Nile to reach their proper goal; when on the far plateau stood no town save of their own so far; as did N’kai on your great plain, where no man-goat makes their way or other land creature visits, or to which certain persons return..., so that\n",
            "******************************\n",
            "4: The ancients thought their legends told strange but plain-smelling secrets, as if the sea had a rich cellar beneath beneath which hidden magic and lore lay unsuspected and unsimed among them. Attach Attach Attas Attached Attak Attached Capt. F., S., with A., C....: Battalions of soldiers! Mr. (L), Attached ‘ Capt\n",
            "******************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LOmPWdppdAly"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}